{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCS 484 Final Project\\n\\nAlexander Wei, a6wei@uwaterloo.ca, 20836214\\nKaiz Nanji\\n\\nProject #5\\nSemi-supervised image classification: assuming that only M out of N images in the traingin data have ground truth labels,\\ndesign and implement a weakly supervsied training of classification network that can benefit from unlabeled examples in\\nthe training dataset(e.g. MNIST or CIFAR-10, but you need to ignore labels on a subset of training examples).You should\\ndemonstrate how the performance changes as M gets progressively smaller. While you can use any well-motivated ideas, one\\nbasic approach could be to combine cross-entropy on labeled points with (unsupervised) K-means clustering loss over deep\\nfeatures (e.g. in the last layer before the linear classifier). It is also advisable to use augmentation (a loss\\nenforcing consistent labeling of augmented training examples). You can also explore Mutual Information loss function\\nformulated in Bridle & MacKay \"Unsupervised Classifiers, Mutual Information and Phantom Targets\", NIPS 1991. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CS 484 Final Project\n",
    "\n",
    "Alexander Wei, a6wei@uwaterloo.ca, 20836214\n",
    "Kaiz Nanji\n",
    "\n",
    "Project #5\n",
    "Semi-supervised image classification: assuming that only M out of N images in the traingin data have ground truth labels,\n",
    "design and implement a weakly supervsied training of classification network that can benefit from unlabeled examples in\n",
    "the training dataset(e.g. MNIST or CIFAR-10, but you need to ignore labels on a subset of training examples).You should\n",
    "demonstrate how the performance changes as M gets progressively smaller. While you can use any well-motivated ideas, one\n",
    "basic approach could be to combine cross-entropy on labeled points with (unsupervised) K-means clustering loss over deep\n",
    "features (e.g. in the last layer before the linear classifier). It is also advisable to use augmentation (a loss\n",
    "enforcing consistent labeling of augmented training examples). You can also explore Mutual Information loss function\n",
    "formulated in Bridle & MacKay \"Unsupervised Classifiers, Mutual Information and Phantom Targets\", NIPS 1991. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import *\n",
    "from src.utils import *\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks the filepath \"./data/MNIST/raw\" for the dataset. If not found, downloads the dataset\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train, mnist_test = download(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase TRAIN_BATCH_SIZE if you are using GPU to speed up training. \n",
    "# When batch size changes, the learning rate may also need to be adjusted. \n",
    "# Note that batch size maybe limited by your GPU memory, so adjust if you get \"run out of GPU memory\" error.\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "\n",
    "# If you are NOT using Windows, set NUM_WORKERS to anything you want, e.g. NUM_WORKERS = 4,\n",
    "# but Windows has issues with multi-process dataloaders, so NUM_WORKERS must be 0 for Windows.\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "val_loader = DataLoader(mnist_test, batch_size=1, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gmm = MyGaussianMixture()\n",
    "my_gmm.clear()\n",
    "\n",
    "my_loss_function = nn.CrossEntropyLoss()\n",
    "my_loss_function = my_loss_function.to(device)\n",
    "\n",
    "my_model = MyModel(device, my_gmm, my_loss_function)\n",
    "my_model = my_model.to(device)\n",
    "\n",
    "sgd_optimizer = get_optimizer(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_graph \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\School\\CS 484\\cs484-final-project\\src\\model.py:92\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, device, train_loader, optimizer, loss_graph)\u001b[0m\n\u001b[0;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(data, target)\n\u001b[0;32m     91\u001b[0m loss_graph\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 92\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_graph[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mg:\\School\\CS 484\\cs484-final-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\School\\CS 484\\cs484-final-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss_graph = []\n",
    "\n",
    "loss = train_model(my_model, device, train_loader, sgd_optimizer, loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "plt.plot(loss_graph)\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.title(\"Training Loss Over Batch Number\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, accurate, confusion_matrix = validate_model(my_model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss, accurate)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[i for i in range(10)])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
