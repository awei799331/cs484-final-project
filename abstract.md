# Abstract

This project tackles the challenge of semi-supervised image classification, aiming to leverage the power of unlabeled data alongside a limited set of labeled examples. We focus on the MNIST handwritten digit classification task, where only a portion of the training images possess ground truth labels. To achieve this, we propose a methodology that combines K-means clustering with an encoder model.

The auto-encoder neural network is an unsupervised learning technique that reduces the dimensionality of the data while attempting to extract relevant features from the data. The initial dimensionality of the MNIST dataset is 1\*28\*28 = 784, and the auto-encoder outputs a dimensionality of 64\*2\*2 = 256. The model is optimized against pixel-wise Mean Squared Error (MSE) loss. For this project, varying amounts of the total training data is used to train the auto-encoder. After training the auto-encoder, both the labeled and unlabeled dataset images are encoded. 

The encodings of the labeled training dataset are used to determine the initial centroids of the K-Means classifier. The hope is that a well-trained auto-encoder is able to successfully separate the encoded dataset into distinct, non-overlapping clusters. Ideally, the Euclidean distance between the encodings of two different digits is large, and the Euclidean distance between the encoding of the same digit is small. The K-Means classifier is then fitted with all labeled and unlabeled encoded data points.

By effectively combining these supervised and unsupervised learning components, our approach aims to improve classification performance even when labeled data is scarce.